{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from slim_gsgp.datasets.data_loader import *\n",
    "from slim_gsgp.main_gp import gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_color = 'blue'\n",
    "test_color = 'orange'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "## Data simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "In addition to the real datasets, simulated data can be a good approach for exploring the algorithms. As an example, following equation generates a dataset with three features ($x_1, x_2, x_3$) that are used to generate the target values ($f(X)$ with some randomness from a Normal distribution($N(0, 3)$):\n",
    "\n",
    "$$\n",
    "f(X) = x_1^2 + x_2^2 + x_3^2 + x_1 x_2 x_3 + N(0, 3)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "\n",
    "feature_1 = np.random.uniform(-10, 10, size=n_samples)\n",
    "feature_2 = np.random.normal(0, 5, size=n_samples)\n",
    "feature_3 = np.random.beta(2, 5, size=n_samples) * 20\n",
    "noise = np.random.normal(0, 3, size=n_samples)\n",
    "\n",
    "target = (\n",
    "    feature_1**2 + feature_2**2 + feature_3**2 + \n",
    "    feature_1 * feature_2 * feature_3 +\n",
    "    np.exp(feature_1) +\n",
    "    noise\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': feature_1,\n",
    "    'feature_2': feature_2,\n",
    "    'feature_3': feature_3,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=seed, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "# Exploring the slim_gsgp library\n",
    "\n",
    "\n",
    "To begin exploring the slim_gsgp library, the first recommended resource is the official documentation: [Slim Documentation](https://slim-library.readthedocs.io/en/latest/).\n",
    "\n",
    "Reading through the source code is also highly informative. _How should you navigate the slim_gsgp source code?_\n",
    "\n",
    "<br />\n",
    "\n",
    "<center>\n",
    "    <img src='slim_framework.png' width=650 />\n",
    "    <br />\n",
    "    Figure 01. Overvoew of the slim_gsgp framwork.\n",
    "</center>\n",
    "\n",
    "- To run an algorithm, use the method named after the algorithm in its **main script (MAIN module in Figure 01)**. For example, to explore GP, open the main GP script: [https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/main_gp.py](https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/main_gp.py).\n",
    "- This main method **instantiates an object of the algorithm's class (ALGORITHMS module in Figure 01)**. The class implementation can be found in the file named after the algorithm. For GP, see: [https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/algorithms/GP/gp.py](https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/algorithms/GP/gp.py).\n",
    "- Finally, it can be helpful to inspect the **configuration file (CONFIG module in Figure 01)** for the algorithm. These files contain, for example, the default hyperparameter settings. For GP, refer to: [https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/config/gp_config.py](https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/config/gp_config.py).\n",
    "\n",
    "##### These steps are also recommended for the other algorithms of the library.\n",
    "\n",
    "<br />\n",
    "\n",
    "##### _How to extend the library (implement your own methods or modify it?_\n",
    "You can either create a branch on the library github repository or download the source codes and work locally. Check the **Developer tutorial** for instructions on how to modify the library: [https://github.com/DALabNOVA/slim/blob/main/CONTRIBUTING.md](https://github.com/DALabNOVA/slim/blob/main/CONTRIBUTING.md).\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "# GP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When solving a symbolic regression problem with GP, it is a good practice to start by defining the problem instance and the search space. This is what it is being done in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Problem Instance definition\n",
    "\n",
    "- `X` and `y`: which dataset will be used?\n",
    "- `fitnesss_function`: the fitness function that will be used to measure the algorithm learning.\n",
    "- `minimization`: is this a minimization problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'syn'\n",
    "# DATASET = 'boston'\n",
    "DATASET = 'bike' # https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset\n",
    "\n",
    "if DATASET == 'syn':\n",
    "    X = torch.tensor(df.values[:, :3], dtype=torch.float32)\n",
    "    y = torch.tensor(df.values[:, 3], dtype=torch.float32)\n",
    "    DATASET_NAME = 'Synthetic'\n",
    "elif DATASET == 'boston':\n",
    "    X, y = load_boston(X_y=True)\n",
    "    DATASET_NAME = 'Boston'\n",
    "elif DATASET == 'bike':\n",
    "    X, y = load_bike_sharing(X_y=True)\n",
    "    # X = X[:, :11]\n",
    "    DATASET_NAME = 'Bike'\n",
    "    \n",
    "FITNESS_FUNCTION = 'rmse'\n",
    "MINIMIZATION = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv = [[train_ix, test_ix] for train_ix, test_ix in cv.split(X, y)][0]\n",
    "\n",
    "# Train and test split\n",
    "X_train_tensor = X[data_cv[0], :]\n",
    "y_train_tensor = y[data_cv[0]]\n",
    "X_val_tensor = X[data_cv[1], :]\n",
    "y_val_tensor = y[data_cv[1]]\n",
    "\n",
    "[X_train_tensor.shape, y_train_tensor.shape, X_val_tensor.shape, y_val_tensor.shape]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Search space definition\n",
    "\n",
    "- `initializer`: how new random trees are initialized. See [`slim_gsgp` initializers](https://github.com/DALabNOVA/slim/blob/main/slim_gsgp/initializers/initializers.py);\n",
    "- `tree_constants`: the constants to be used in the terminal set;\n",
    "- `tree_functions`: the function set (tree internal nodes);\n",
    "- `prob_const`: the probability for choosing constants instead of dataset features on tree terminals;\n",
    "- `init_depth`: max depth for tree initialisation;\n",
    "- `max_depth`: max depth of trees during algorithm evolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIALIZER = 'grow'\n",
    "TREE_CONSTANTS = [random.uniform(0, 1) for _ in range(9)]+[ -1.]\n",
    "TREE_FUNCTIONS = ['add', 'subtract']\n",
    "PROB_CONSTANT = 0.9\n",
    "MAX_INIT_DEPTH = 4\n",
    "MAX_DEPTH = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREE_CONSTANTS\n",
    "\n",
    "# [0.21760077176688164,\n",
    "#  0.3443807346030824,\n",
    "#  0.6422536234699076,\n",
    "#  0.36413206493253214,\n",
    "#  0.08358916437841302,\n",
    "#  0.5040914040192876,\n",
    "#  0.18743462930144428,\n",
    "#  0.8842252761132199,\n",
    "#  0.33821341140965044,\n",
    "#  -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GP Instance\n",
    "\n",
    "It is library-dependent. On `slim_gsgp`, the following customization options are available:\n",
    "\n",
    "- `pop_size`: the size of the population of candidate solutions.\n",
    "- `p_xo`: the probability of applying the cross-over genetic operator to candidate solutions.\n",
    "- `elitism`: should the elite(s) be preserved at each generation?\n",
    "- `n_elits`: if using elitism, how many solutions should be kept?\n",
    "- Selection method. Only tournament selection in available on `slim_gsgp` libraya, as this is the most commonly used. It requires the definition of the `tournament_size` hyperparameter: **how many solutions should participate in the tournament of tournament selection?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_SIZE = 50\n",
    "P_XO = 0.9\n",
    "ELISTISM = True\n",
    "N_ELITES = 1\n",
    "TOURNAMENT_SIZE = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Solve settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIONS = 30\n",
    "VERBOSE = 1\n",
    "\n",
    "# Log level 2\n",
    "# -----------\n",
    "# 0  - Algorithm\n",
    "# 1  - Instance ID\n",
    "# 2  - Dataset\n",
    "# 3  - Seed\n",
    "# 4  - Generation\n",
    "# 5  - Fitness\n",
    "# 6  - Running time\n",
    "# 7  - Population nodes\n",
    "# 8  - Test fitness\n",
    "# 9  - Elite nodes\n",
    "# 10 - Genotype diversity: niche entropy\n",
    "# 11 - Phenotype diversity: sd(pop.fit)\n",
    "# 12 - Log level\n",
    "LOG_LEVEL = 2\n",
    "LOG_DIR = './log/PC2/'\n",
    "LOG_PATH = LOG_DIR+'gp_'+DATASET_NAME+'.csv'\n",
    "\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "if os.path.exists(LOG_PATH):\n",
    "    os.remove(LOG_PATH)\n",
    "\n",
    "print(f'Total evaluations: {POP_SIZE*GENERATIONS}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gp(\n",
    "    # ---\n",
    "    # Search Space\n",
    "    init_depth=MAX_INIT_DEPTH,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    tree_constants=TREE_CONSTANTS,\n",
    "    tree_functions=TREE_FUNCTIONS,\n",
    "    prob_const = PROB_CONSTANT,\n",
    "    # --\n",
    "    # Problem Instance\n",
    "    X_train=X_train_tensor, y_train=y_train_tensor, \n",
    "    X_test=X_val_tensor, y_test=y_val_tensor,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    fitness_function=FITNESS_FUNCTION,\n",
    "    minimization=MINIMIZATION,\n",
    "    # --\n",
    "    # GP instance \n",
    "    pop_size=POP_SIZE,\n",
    "    p_xo = P_XO,\n",
    "    initializer=INITIALIZER,\n",
    "    tournament_size = TOURNAMENT_SIZE,\n",
    "    # ---\n",
    "    # Solve settings\n",
    "    n_iter=GENERATIONS,\n",
    "    elitism=ELISTISM,\n",
    "    n_elites=N_ELITES,\n",
    "    test_elite=True,\n",
    "    log_path=LOG_PATH,\n",
    "    log_level=LOG_LEVEL,\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=1,\n",
    "    seed=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(LOG_PATH, header=None).head()\n",
    "# 0  - Algorithm\n",
    "# 1  - Instance ID\n",
    "# 2  - Dataset\n",
    "# 3  - Seed\n",
    "# 4  - Generation\n",
    "# 5  - Fitness\n",
    "# 6  - Running time\n",
    "# 7  - Population nodes\n",
    "# 8  - Test fitness\n",
    "# 9  - Elite nodes\n",
    "# 10 - niche entropy\n",
    "# 11 - sd(pop.fit)\n",
    "# 12 - Log level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,5].values, \n",
    "                         mode='lines', name='Train', line=dict(color=train_color)))\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,8].values, \n",
    "                         mode='lines', name='Test', line=dict(color=test_color)))\n",
    "fig.update_layout(\n",
    "    height=400, width=800, \n",
    "    margin=dict(t=50),\n",
    "    yaxis_range=[0,None],\n",
    "    title_text='GP - Train vs Test Fitness ('+DATASET_NAME+' dataset)',\n",
    "    xaxis_title='Generation', yaxis_title='RMSE'\n",
    ")\n",
    "fig.update_yaxes(range=[0, None])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('GP - Fitness evolution ('+DATASET_NAME+' dataset)', 'GP - Size evolution ('+DATASET_NAME+' dataset)')\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,5].values, \n",
    "                         mode='lines', name='Train', line=dict(color=train_color)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,8].values, \n",
    "                         mode='lines', name='Test', line=dict(color=test_color)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,9].values, \n",
    "                         mode='lines', name='Size'), row=1, col=2)\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=400, \n",
    "    showlegend=True,\n",
    "    yaxis_range=[0,None],\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=-0.3,\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_tree_representation()\n",
    "# model.node_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,10].values, \n",
    "                         mode='lines', name='Train', line=dict(color=train_color)))\n",
    "fig.update_layout(\n",
    "    height=400, width=800, \n",
    "    margin=dict(t=50),\n",
    "    title_text='GP - Niche entropy ('+DATASET_NAME+' dataset)',\n",
    "    yaxis_range=[0,None],\n",
    "    xaxis_title='Generation', yaxis_title='Entropy'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=pd.read_csv(LOG_PATH, header=None).iloc[:,11].values, \n",
    "                         mode='lines', name='Train', line=dict(color=train_color)))\n",
    "fig.update_layout(\n",
    "    height=400, width=800, \n",
    "    margin=dict(t=50),\n",
    "    yaxis_range=[0,None],\n",
    "    title_text='GP - Population Fitness Diversity ('+DATASET_NAME+' dataset)',\n",
    "    xaxis_title='Generation', yaxis_title='Fitness Standard Deviation'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_val_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.fitness, model.test_fitness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<hr />\n",
    "\n",
    "# Excercises (not graded)\n",
    "\n",
    "- Experiment new datasets.\n",
    "- Run the hyperparameters tunning for the Boston or synthetic data.\n",
    "    \n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "xai_clustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
